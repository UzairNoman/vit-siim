{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import argparse\n",
    "from sklearn import metrics\n",
    "import comet_ml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import pytorch_lightning as pl\n",
    "import warmup_scheduler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "import torchmetrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from utils import get_model, get_dataset, get_experiment_name, get_criterion\n",
    "from da import CutMix, MixUp\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "class Settings:\n",
    "    def __init__(self):\n",
    "        self.dataset = \"siim\"\n",
    "        self.num_classes = 2\n",
    "        self.model_name = \"vit\"\n",
    "        self.patch = 8\n",
    "        self.batch_size = 8\n",
    "        self.eval_batch_size = 1024\n",
    "        self.lr = 1e-3\n",
    "        self.min_lr = 1e-5\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.999\n",
    "        self.max_epochs = 3\n",
    "        self.weight_decay = 5e-5\n",
    "        self.warmup_epoch = 5\n",
    "        self.precision = 16\n",
    "        self.criterion = \"ce\"\n",
    "        self.smoothing = 0.1\n",
    "        self.dropout = 0.0\n",
    "        self.head = 12\n",
    "        self.num_layers = 7\n",
    "        self.hidden = 384\n",
    "        self.label_smoothing = False\n",
    "        self.mlp_hidden = 384\n",
    "        self.seed = 42\n",
    "        self.project_name = \"VisionTransformer\"\n",
    "        self.off_benchmark = False\n",
    "        self.dry_run = False\n",
    "        self.autoaugment = False\n",
    "        self.rcpaste = False\n",
    "        self.cutmix = False\n",
    "        self.mixup = False\n",
    "        self.off_cls_token = False\n",
    "        self.api_key = False\n",
    "\n",
    "args = Settings()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "args.benchmark = True if not args.off_benchmark else False\n",
    "args.gpus = torch.cuda.device_count()\n",
    "args.num_workers = 4*args.gpus if args.gpus else 8\n",
    "args.is_cls_token = True if not args.off_cls_token else False\n",
    "if not args.gpus:\n",
    "    args.precision=32\n",
    "\n",
    "if args.mlp_hidden != args.hidden*4:\n",
    "    print(f\"[INFO] In original paper, mlp_hidden(CURRENT:{args.mlp_hidden}) is set to: {args.hidden*4}(={args.hidden}*4)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import argparse\n",
    "from sklearn import metrics\n",
    "#import comet_ml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import pytorch_lightning as pl\n",
    "import warmup_scheduler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "import torchmetrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from utils import get_model, get_dataset, get_experiment_name, get_criterion\n",
    "from da import CutMix, MixUp\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from ham import HAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "#import torchvision\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class HAM(Dataset):\n",
    "    \"\"\"HAM dataset class\"\"\"\n",
    "\n",
    "    def __init__(self, root, purpose, seed, split, transforms=None, tfm_on_patch=None):\n",
    "        self.root_path = root\n",
    "        self.purpose = purpose\n",
    "        self.seed = seed\n",
    "        self.split = split\n",
    "        self.img_part1 = os.listdir(f'{root}/HAM10000_images_part_1/')\n",
    "        self.img_part2 = os.listdir(f'{root}/HAM10000_images_part_2/')\n",
    "        self.images, self.labels = self._make_dataset(directory=self.root_path, purpose=self.purpose, seed=self.seed, split=self.split)\n",
    "        self.transforms = transforms\n",
    "        self.tfm_on_patch = tfm_on_patch\n",
    "\n",
    "    def _make_dataset(self,directory, purpose, seed, split):\n",
    "        \"\"\"\n",
    "        Create the image dataset by preparing a list of samples\n",
    "        :param directory: root directory of the dataset\n",
    "        :returns: (images, labels) where:\n",
    "            - images is a numpy array containing all images in the dataset\n",
    "            - labels is a list containing one label per image\n",
    "        \"\"\"\n",
    "\n",
    "        data_path = os.path.join(directory, \"HAM10000_metadata.csv\")\n",
    "        meta_df = pd.read_csv(data_path)\n",
    "        # meta_df.rename(columns={'image_id': 'image_name'})\n",
    "        meta_df['target'] = pd.Categorical(meta_df['dx']).codes\n",
    "        no_of_classes = meta_df['target'].unique()\n",
    "        print(f'No. of Class in HAM: {no_of_classes}')\n",
    "        meta_df['image_name'] = meta_df.apply(lambda row: self.extract_path_img(directory,row.image_id), axis=1)\n",
    "\n",
    "    \n",
    "        #(33126, 8)\n",
    "        \n",
    "        train, val = train_test_split(meta_df, test_size=split, random_state=seed)\n",
    "        #do we want to apply stratification here?\n",
    "        # train, val, test = np.split(meta_df.sample(frac=1, random_state=seed), \n",
    "        #                                 [int(split*meta_df.shape[0]), int(((1.0-split)/2.0+split)*meta_df.shape[0])])\n",
    "\n",
    "        #train -> 24844\n",
    "        #val -> 8282\n",
    "        # trueRows = train[train['target'] == 1] # 434\n",
    "        # falseRows = train[train['target'] == 0] # 24410\n",
    "        # # # print(len(trueRows))\n",
    "        # # # print(f\" = > {len(falseRows) - len(trueRows)}\")\n",
    "        # trueReplicas = pd.concat([trueRows]*(math.ceil(len(falseRows)/len(trueRows)))) # 434*57 = 24738\n",
    "\n",
    "        print(train['target'])\n",
    "        \n",
    "        \n",
    "        # oversampled = falseRows.append(trueReplicas[:len(falseRows) - len(trueRows)], ignore_index=True) # 24410 + 23976  = 48386\n",
    "        ######################\n",
    "\n",
    "        if purpose=='train':\n",
    "            return train['image_name'].tolist(), train['target'].tolist()\n",
    "        elif purpose=='val':\n",
    "            return val['image_name'].tolist(), val['target'].tolist()\n",
    "        elif purpose=='test':\n",
    "            data_path = os.path.join(directory, \"test.csv\")\n",
    "            test_df = pd.read_csv(data_path, sep=',')\n",
    "\n",
    "            return test_df['image_name'].tolist(), []\n",
    "\n",
    "    def extract_path_img(self,directory,x):\n",
    "        file = x + '.jpg'\n",
    "        \n",
    "        if file in self.img_part1:\n",
    "            \n",
    "            return os.path.join(f'{directory}/HAM10000_images_part_1', file)\n",
    "        \n",
    "        elif file in self.img_part2:\n",
    "            \n",
    "            return os.path.join(f'{directory}/HAM10000_images_part_2', file)\n",
    "    def __len__(self):\n",
    "        \"\"\"Return number of images in the dataset\"\"\"\n",
    "        return(len(self.images))\n",
    "        \n",
    "    def get_labels(self): return self.labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Creates a dict of the data at the given index:\n",
    "            {\"image\": <i-th image>,                                              #\n",
    "             \"label\": <label of i-th image>} \n",
    "        \"\"\"\n",
    "        img_root = self.images[index]\n",
    "        \n",
    "        #img = Image.open(img_root)\n",
    "        #trans = transforms.ToTensor()\n",
    "        #img = trans(img)\n",
    "        #img = torchvision.io.read_image(img_root)\n",
    "        img = read_image(img_root)\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "\n",
    "        if self.labels[index] == 1:\n",
    "            transformForReplicas = transforms.RandomChoice([\n",
    "                transforms.RandomHorizontalFlip(), \n",
    "                transforms.RandomVerticalFlip(),\n",
    "                transforms.RandomAutocontrast(),\n",
    "                transforms.RandomAdjustSharpness(sharpness_factor=2),\n",
    "                transforms.RandomEqualize()\n",
    "            ])\n",
    "\n",
    "            transformForReplicas2 = transforms.RandomChoice([\n",
    "                transforms.ColorJitter(brightness=.5, hue=.3),\n",
    "                transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n",
    "                transforms.RandomRotation(degrees=(0, 180)),\n",
    "                transforms.RandomPosterize(bits=2),\n",
    "                \n",
    "            ])\n",
    "\n",
    "            img = transformForReplicas(img)\n",
    "            img = transformForReplicas2(img)\n",
    "        img = img.float()\n",
    "     \n",
    "        if self.purpose == 'test':\n",
    "            return img\n",
    "        else:\n",
    "            return img, self.labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_path_img(directory,x):\n",
    "    file = x + '.jpg'\n",
    "        \n",
    "    if file in os.listdir(f'{directory}/HAM10000_images_part_1/'):\n",
    "            \n",
    "        return os.path.join(f'{directory}/HAM10000_images_part_1', file)\n",
    "        \n",
    "    elif file in os.listdir(f'{directory}/HAM10000_images_part_2/'):\n",
    "            \n",
    "        return os.path.join(f'{directory}/HAM10000_images_part_2', file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Class in HAM: [2 5 3 4 6 1 0]\n",
      "4219    5\n",
      "9092    5\n",
      "8016    5\n",
      "3292    5\n",
      "819     2\n",
      "       ..\n",
      "5734    5\n",
      "5191    5\n",
      "5390    5\n",
      "860     2\n",
      "7270    5\n",
      "Name: target, Length: 2003, dtype: int8\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "base_path = \"/Users/k.urbanczyk/Desktop/archive\"\n",
    "#train_ds = HAM(base_path, purpose='train', seed=42, split=0.25)\n",
    "\n",
    "\n",
    "data_path = os.path.join(base_path, \"HAM10000_metadata.csv\")\n",
    "meta_df = pd.read_csv(data_path)\n",
    "# meta_df.rename(columns={'image_id': 'image_name'})\n",
    "meta_df['target'] = pd.Categorical(meta_df['dx']).codes\n",
    "no_of_classes = meta_df['target'].unique()\n",
    "print(f'No. of Class in HAM: {no_of_classes}')\n",
    "meta_df['image_name'] = meta_df.apply(lambda row: extract_path_img(base_path, row.image_id), axis=1)\n",
    "\n",
    "    \n",
    "#(33126, 8)\n",
    "        \n",
    "train, val = train_test_split(meta_df, test_size=0.8, random_state=42)\n",
    "print(train['target'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "1355\n",
      "5    1355\n",
      "3    1336\n",
      "6    1329\n",
      "0    1294\n",
      "1    1248\n",
      "4    1144\n",
      "2    1131\n",
      "Name: target, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s3/jrl64rw543d61yb108hw2jlr0000gn/T/ipykernel_32399/1755862149.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  oversampled = oversampled.append(replicas[:most_popular_count - len(to_be_replicated)], ignore_index=True)\n",
      "/var/folders/s3/jrl64rw543d61yb108hw2jlr0000gn/T/ipykernel_32399/1755862149.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  oversampled = oversampled.append(replicas[:most_popular_count - len(to_be_replicated)], ignore_index=True)\n",
      "/var/folders/s3/jrl64rw543d61yb108hw2jlr0000gn/T/ipykernel_32399/1755862149.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  oversampled = oversampled.append(replicas[:most_popular_count - len(to_be_replicated)], ignore_index=True)\n",
      "/var/folders/s3/jrl64rw543d61yb108hw2jlr0000gn/T/ipykernel_32399/1755862149.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  oversampled = oversampled.append(replicas[:most_popular_count - len(to_be_replicated)], ignore_index=True)\n",
      "/var/folders/s3/jrl64rw543d61yb108hw2jlr0000gn/T/ipykernel_32399/1755862149.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  oversampled = oversampled.append(replicas[:most_popular_count - len(to_be_replicated)], ignore_index=True)\n",
      "/var/folders/s3/jrl64rw543d61yb108hw2jlr0000gn/T/ipykernel_32399/1755862149.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  oversampled = oversampled.append(replicas[:most_popular_count - len(to_be_replicated)], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "most_popular_count = train['target'].value_counts().max()\n",
    "most_popular_idx = train['target'].value_counts().idxmax()\n",
    "oversampled = train[train['target'] == most_popular_idx]\n",
    "print(most_popular_idx)\n",
    "print(most_popular_count)\n",
    "for i in range(0,7):\n",
    "    if most_popular_idx != i:\n",
    "        to_be_replicated = train[train['target'] == i]\n",
    "        replicas = pd.concat([to_be_replicated]*(math.ceil(most_popular_count/len(to_be_replicated))))\n",
    "        oversampled = oversampled.append(replicas[:most_popular_count - len(to_be_replicated)], ignore_index=True)\n",
    "print(oversampled['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HAM():\n",
    "    \"\"\"HAM dataset class\"\"\"\n",
    "\n",
    "    def __init__(self, root, purpose, seed, split, transforms=None, tfm_on_patch=None):\n",
    "        self.root_path = root\n",
    "        self.purpose = purpose\n",
    "        self.seed = seed\n",
    "        self.split = split\n",
    "        self.img_part1 = os.listdir(f'{root}/HAM10000_images_part_1/')\n",
    "        self.img_part2 = os.listdir(f'{root}/HAM10000_images_part_2/')\n",
    "        self.images, self.labels, self.most_popular_idx = self._make_dataset(directory=self.root_path, purpose=self.purpose, seed=self.seed, split=self.split)\n",
    "        self.transforms = transforms\n",
    "        self.tfm_on_patch = tfm_on_patch\n",
    "\n",
    "    def _make_dataset(self,directory, purpose, seed, split):\n",
    "        \"\"\"\n",
    "        Create the image dataset by preparing a list of samples\n",
    "        :param directory: root directory of the dataset\n",
    "        :returns: (images, labels) where:\n",
    "            - images is a numpy array containing all images in the dataset\n",
    "            - labels is a list containing one label per image\n",
    "        \"\"\"\n",
    "\n",
    "        data_path = os.path.join(directory, \"HAM10000_metadata.csv\")\n",
    "        meta_df = pd.read_csv(data_path)\n",
    "        # meta_df.rename(columns={'image_id': 'image_name'})\n",
    "        meta_df['target'] = pd.Categorical(meta_df['dx']).codes\n",
    "        no_of_classes = meta_df['target'].unique()\n",
    "        #print(f'No. of Class in HAM: {no_of_classes}')\n",
    "        meta_df['image_name'] = meta_df.apply(lambda row: self.extract_path_img(directory,row.image_id), axis=1)\n",
    "\n",
    "    \n",
    "        #(33126, 8)\n",
    "        \n",
    "        train, val = train_test_split(meta_df, test_size=split, random_state=seed)\n",
    "        #do we want to apply stratification here?\n",
    "        # train, val, test = np.split(meta_df.sample(frac=1, random_state=seed), \n",
    "        #                                 [int(split*meta_df.shape[0]), int(((1.0-split)/2.0+split)*meta_df.shape[0])])\n",
    "\n",
    "        #train -> 24844\n",
    "        #val -> 8282\n",
    "        # trueRows = train[train['target'] == 1] # 434\n",
    "        # falseRows = train[train['target'] == 0] # 24410\n",
    "        # # # print(len(trueRows))\n",
    "        # # # print(f\" = > {len(falseRows) - len(trueRows)}\")\n",
    "        # trueReplicas = pd.concat([trueRows]*(math.ceil(len(falseRows)/len(trueRows)))) # 434*57 = 24738\n",
    "\n",
    "        #print(train['target'])\n",
    "        most_popular_count = train['target'].value_counts().max()\n",
    "        most_popular_idx = train['target'].value_counts().idxmax()\n",
    "        oversampled = train[train['target'] == most_popular_idx]\n",
    "        #print(most_popular_idx)\n",
    "        #print(most_popular_count)\n",
    "        for i in range(0,7):\n",
    "            if most_popular_idx != i:\n",
    "                to_be_replicated = train[train['target'] == i]\n",
    "                replicas = pd.concat([to_be_replicated]*(math.ceil(most_popular_count/len(to_be_replicated))))\n",
    "                oversampled = oversampled.append(replicas[:most_popular_count - len(to_be_replicated)], ignore_index=True)\n",
    "        #print(oversampled['target'].value_counts())\n",
    "        \n",
    "        # oversampled = falseRows.append(trueReplicas[:len(falseRows) - len(trueRows)], ignore_index=True) # 24410 + 23976  = 48386\n",
    "        ######################\n",
    "\n",
    "        if purpose=='train':\n",
    "            return oversampled['image_name'].tolist(), oversampled['target'].tolist(), most_popular_idx\n",
    "        elif purpose=='val':\n",
    "            return val['image_name'].tolist(), val['target'].tolist(), most_popular_idx\n",
    "        elif purpose=='test':\n",
    "            data_path = os.path.join(directory, \"test.csv\")\n",
    "            test_df = pd.read_csv(data_path, sep=',')\n",
    "\n",
    "            return test_df['image_name'].tolist(), [], most_popular_idx\n",
    "\n",
    "    def extract_path_img(self,directory,x):\n",
    "        file = x + '.jpg'\n",
    "        \n",
    "        if file in self.img_part1:\n",
    "            \n",
    "            return os.path.join(f'{directory}/HAM10000_images_part_1', file)\n",
    "        \n",
    "        elif file in self.img_part2:\n",
    "            \n",
    "            return os.path.join(f'{directory}/HAM10000_images_part_2', file)\n",
    "    def __len__(self):\n",
    "        \"\"\"Return number of images in the dataset\"\"\"\n",
    "        return(len(self.images))\n",
    "        \n",
    "    def get_labels(self): return self.labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Creates a dict of the data at the given index:\n",
    "            {\"image\": <i-th image>,                                              #\n",
    "             \"label\": <label of i-th image>} \n",
    "        \"\"\"\n",
    "        img_root = self.images[index]\n",
    "        \n",
    "        #img = Image.open(img_root)\n",
    "        #trans = transforms.ToTensor()\n",
    "        #img = trans(img)\n",
    "        #img = torchvision.io.read_image(img_root)\n",
    "        img = read_image(img_root)\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "\n",
    "        if self.labels[index] != self.most_popular_idx:\n",
    "            transformForReplicas = transforms.RandomChoice([\n",
    "                transforms.RandomHorizontalFlip(), \n",
    "                transforms.RandomVerticalFlip(),\n",
    "                transforms.RandomAutocontrast(),\n",
    "                transforms.RandomAdjustSharpness(sharpness_factor=2),\n",
    "                transforms.RandomEqualize()\n",
    "            ])\n",
    "\n",
    "            transformForReplicas2 = transforms.RandomChoice([\n",
    "                transforms.ColorJitter(brightness=.5, hue=.3),\n",
    "                transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n",
    "                transforms.RandomRotation(degrees=(0, 180)),\n",
    "                transforms.RandomPosterize(bits=2),\n",
    "                \n",
    "            ])\n",
    "\n",
    "            img = transformForReplicas(img)\n",
    "            img = transformForReplicas2(img)\n",
    "        img = img.float()\n",
    "     \n",
    "        if self.purpose == 'test':\n",
    "            return img\n",
    "        else:\n",
    "            return img, self.labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s3/jrl64rw543d61yb108hw2jlr0000gn/T/ipykernel_47284/2581524604.py:58: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  oversampled = oversampled.append(replicas[:most_popular_count - len(to_be_replicated)], ignore_index=True)\n",
      "/var/folders/s3/jrl64rw543d61yb108hw2jlr0000gn/T/ipykernel_47284/2581524604.py:58: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  oversampled = oversampled.append(replicas[:most_popular_count - len(to_be_replicated)], ignore_index=True)\n",
      "/var/folders/s3/jrl64rw543d61yb108hw2jlr0000gn/T/ipykernel_47284/2581524604.py:58: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  oversampled = oversampled.append(replicas[:most_popular_count - len(to_be_replicated)], ignore_index=True)\n",
      "/var/folders/s3/jrl64rw543d61yb108hw2jlr0000gn/T/ipykernel_47284/2581524604.py:58: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  oversampled = oversampled.append(replicas[:most_popular_count - len(to_be_replicated)], ignore_index=True)\n",
      "/var/folders/s3/jrl64rw543d61yb108hw2jlr0000gn/T/ipykernel_47284/2581524604.py:58: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  oversampled = oversampled.append(replicas[:most_popular_count - len(to_be_replicated)], ignore_index=True)\n",
      "/var/folders/s3/jrl64rw543d61yb108hw2jlr0000gn/T/ipykernel_47284/2581524604.py:58: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  oversampled = oversampled.append(replicas[:most_popular_count - len(to_be_replicated)], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "base_path = \"/Users/k.urbanczyk/Desktop/archive\"\n",
    "train_ds = HAM(base_path, purpose='train', seed=42, split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_classes = 1\n",
    "N_per_class=100\n",
    "label = np.concatenate([[i]*N_per_class for i in range(N_classes)])\n",
    "out = np.stack([np.random.uniform(0,1,N_per_class*N_classes) for _ in range(N_classes)]).T\n",
    "out /= out.sum(1,keepdims=True) #approximate softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/k.urbanczyk/opt/miniconda3/envs/cenv-vit-siim/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:990: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 1 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/k.urbanczyk/Documents/datascience/datapractical/vit-siim/play.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/k.urbanczyk/Documents/datascience/datapractical/vit-siim/play.ipynb#ch0000036?line=0'>1</a>\u001b[0m plot_multiclass_auc(out, label)\n",
      "\u001b[1;32m/Users/k.urbanczyk/Documents/datascience/datapractical/vit-siim/play.ipynb Cell 13'\u001b[0m in \u001b[0;36mplot_multiclass_auc\u001b[0;34m(out, label, text)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/k.urbanczyk/Documents/datascience/datapractical/vit-siim/play.ipynb#ch0000036?line=2'>3</a>\u001b[0m f,ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/k.urbanczyk/Documents/datascience/datapractical/vit-siim/play.ipynb#ch0000036?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m7\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/k.urbanczyk/Documents/datascience/datapractical/vit-siim/play.ipynb#ch0000036?line=4'>5</a>\u001b[0m     fpr[i], tpr[i], _ \u001b[39m=\u001b[39m roc_curve(label\u001b[39m==\u001b[39mi, out[:, i])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/k.urbanczyk/Documents/datascience/datapractical/vit-siim/play.ipynb#ch0000036?line=5'>6</a>\u001b[0m     roc_auc[i] \u001b[39m=\u001b[39m auc(fpr[i], tpr[i])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/k.urbanczyk/Documents/datascience/datapractical/vit-siim/play.ipynb#ch0000036?line=6'>7</a>\u001b[0m     ax\u001b[39m.\u001b[39mplot(fpr[i],tpr[i])\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 1 with size 1"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_multiclass_auc(out, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multiclass_auc(out,label, text=\"AUC Curve\"):\n",
    "    tpr,fpr,roc_auc = ([[]]*7 for _ in range(3))\n",
    "    f,ax = plt.subplots()\n",
    "    for i in range(7):\n",
    "        fpr[i], tpr[i], _ = roc_curve(label==i, out[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        ax.plot(fpr[i],tpr[i])\n",
    "        f1 = f1_score(torch.tensor(label), torch.argmax(torch.tensor(out), dim=1), average ='micro')\n",
    "    plt.legend(['Class {:d}, AUC {}'.format(d, round(roc_auc[d],3)) for d in range(7)])\n",
    "    plt.title('{}, {}, F1-Score: {})'.format(text,\"gunwo\", f1))\n",
    "    plt.xlabel('FPR')\n",
    "    plt.ylabel('TPR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= roc_curve(label==1, out[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "out2 = torch.round(torch.tensor(out))\n",
    "label2 = torch.tensor(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12857142857142856"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(label2, out2, average ='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(label, out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_curve(label==i, out[:, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'generator' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/k.urbanczyk/Documents/datascience/datapractical/vit-siim/play.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/k.urbanczyk/Documents/datascience/datapractical/vit-siim/play.ipynb#ch0000032?line=0'>1</a>\u001b[0m roc_auc[\u001b[39m0\u001b[39;49m]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'generator' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "roc_auc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "\n",
    "\n",
    "class ImbalancedDatasetSampler(torch.utils.data.sampler.Sampler):\n",
    "    \"\"\"Samples elements randomly from a given list of indices for imbalanced dataset\n",
    "    Arguments:\n",
    "        indices (list, optional): a list of indices\n",
    "        num_samples (int, optional): number of samples to draw\n",
    "        callback_get_label func: a callback-like function which takes two arguments - dataset and index\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, indices=None, num_samples=None, callback_get_label=None):\n",
    "                \n",
    "        # if indices is not provided, \n",
    "        # all elements in the dataset will be considered\n",
    "        self.indices = list(range(len(dataset))) \\\n",
    "            if indices is None else indices\n",
    "\n",
    "        # define custom callback\n",
    "        self.callback_get_label = callback_get_label\n",
    "\n",
    "        # if num_samples is not provided, \n",
    "        # draw `len(indices)` samples in each iteration\n",
    "        self.num_samples = len(self.indices) \\\n",
    "            if num_samples is None else num_samples\n",
    "            \n",
    "        # distribution of classes in the dataset \n",
    "        label_to_count = {}\n",
    "        for idx in self.indices:\n",
    "            label = self._get_label(dataset, idx)\n",
    "            if label in label_to_count:\n",
    "                label_to_count[label] += 1\n",
    "            else:\n",
    "                label_to_count[label] = 1\n",
    "                \n",
    "        # weight for each sample\n",
    "        weights = [1.0 / label_to_count[self._get_label(dataset, idx)]\n",
    "                   for idx in self.indices]\n",
    "        self.weights = torch.DoubleTensor(weights)\n",
    "\n",
    "    def _get_label(self, dataset, idx):\n",
    "        return dataset.train_labels[idx].item()\n",
    "                \n",
    "    def __iter__(self):\n",
    "        return (self.indices[i] for i in torch.multinomial(\n",
    "            self.weights, self.num_samples, replacement=True))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, test_ds = get_dataset(args)\n",
    "# classes = torch.tensor([0, 1, 2])\n",
    "# indices = (torch.tensor(train_ds.targets)[..., None] == classes).any(-1).nonzero(as_tuple=True)[0]\n",
    "# train_ds = torch.utils.data.Subset(train_ds, indices)\n",
    "# test_ds = torch.utils.data.Subset(test_ds, indices)\n",
    "#print(data.shape)\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=args.batch_size, shuffle=True)#, num_workers=args.num_workers, pin_memory=True)\n",
    "test_dl = torch.utils.data.DataLoader(test_ds, batch_size=args.eval_batch_size, num_workers=args.num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsampler import ImbalancedDatasetSampler\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_ds, \n",
    "    sampler=ImbalancedDatasetSampler(train_ds),\n",
    "    batch_size=args.batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "applier = transforms.RandomApply(transforms=[transforms.RandomCrop(size=(64, 64))], p=0.5)\n",
    "transformed_imgs = applier(train_ds[0][0])\n",
    "plt.imshow((train_ds[4][0].permute(1, 2, 0)).numpy().astype(np.uint8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_it_somehow = train_ds[1][0]\n",
    "\n",
    "# img = plot_it_somehow.swapaxes(0, 1)\n",
    "# img = img.swapaxes(1, 2)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow((plot_it_somehow.permute(1, 2, 0)).numpy().astype(np.uint8))\n",
    "plt.show()\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# pil_image=Image.fromarray(plot_it_somehow.permute(1, 2, 0).numpy().astype(np.uint8))\n",
    "# pil_image.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "# sample input (10 RGB images containing just Gaussian Noise)\n",
    "batch_tensor = ex[0]   # (N, C, H, W)\n",
    "\n",
    "# make grid (2 rows and 5 columns) to display our 10 images\n",
    "grid_img = torchvision.utils.make_grid(batch_tensor, nrow=5)\n",
    "\n",
    "grid_img.shape\n",
    "plt.imshow((grid_img.permute(1, 2, 0)).numpy().astype(np.uint8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super(Net, self).__init__()\n",
    "        # self.hparams = hparams\n",
    "        self.hparams.update(vars(hparams))\n",
    "        self.model = get_model(hparams)\n",
    "        self.criterion = get_criterion(args)\n",
    "        if hparams.cutmix:\n",
    "            self.cutmix = CutMix(hparams.size, beta=1.)\n",
    "        if hparams.mixup:\n",
    "            self.mixup = MixUp(alpha=1.)\n",
    "        self.log_image_flag = hparams.api_key is None\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.hparams.lr, betas=(self.hparams.beta1, self.hparams.beta2), weight_decay=self.hparams.weight_decay)\n",
    "        self.base_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=self.hparams.max_epochs, eta_min=self.hparams.min_lr)\n",
    "        self.scheduler = warmup_scheduler.GradualWarmupScheduler(self.optimizer, multiplier=1., total_epoch=self.hparams.warmup_epoch, after_scheduler=self.base_scheduler)\n",
    "        return [self.optimizer], [self.scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        img, label = batch\n",
    "        if self.hparams.cutmix or self.hparams.mixup:\n",
    "            if self.hparams.cutmix:\n",
    "                img, label, rand_label, lambda_= self.cutmix((img, label))\n",
    "            elif self.hparams.mixup:\n",
    "                if np.random.rand() <= 0.8:\n",
    "                    img, label, rand_label, lambda_ = self.mixup((img, label))\n",
    "                else:\n",
    "                    img, label, rand_label, lambda_ = img, label, torch.zeros_like(label), 1.\n",
    "            out = self.model(img)\n",
    "            loss = self.criterion(out, label)*lambda_ + self.criterion(out, rand_label)*(1.-lambda_)\n",
    "        else:\n",
    "            out = self(img)\n",
    "            loss = self.criterion(out[:,1], label.float())\n",
    "\n",
    "        if not self.log_image_flag and not self.hparams.dry_run:\n",
    "            self.log_image_flag = True\n",
    "            #self._log_image(img.clone().detach().cpu())\n",
    "\n",
    "        acc = torch.eq(out.argmax(-1), label).float().mean()\n",
    "        auc_score = metrics.roc_auc_score(label, out[:, 1].squeeze().detach().numpy())\n",
    "        self.log('auc', auc_score, on_step=True, on_epoch=True)\n",
    "        self.log('acc', acc, on_step=True, on_epoch=True)\n",
    "        self.log('loss', loss,on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        #self.log(\"lr\", self.optimizer.param_groups[0][\"lr\"], on_epoch=self.current_epoch)\n",
    "        self.log(\"lr\", self.optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        img, label = batch\n",
    "        out = self(img)\n",
    "        loss = self.criterion(out[:,1], label.float())\n",
    "        acc = torch.eq(out.argmax(-1), label).float().mean()\n",
    "        #self.log(\"val_loss\", loss)\n",
    "        #self.log(\"val_acc\", acc)\n",
    "\n",
    "        auc_score = metrics.roc_auc_score(label.cpu(), out[:, 1].squeeze())\n",
    "        self.log('auc', auc_score, on_step=True, on_epoch=True)\n",
    "        val_acc = torchmetrics.functional.accuracy(out[:, 1], label)\n",
    "        self.log('valid_acc_from_tmet', val_acc, on_step=True, on_epoch=True)\n",
    "        self.log('valid_acc', acc, on_step=True, on_epoch=True)\n",
    "        self.log('val_loss', loss,on_step=True, on_epoch=True)\n",
    "\n",
    "        # fpr, tpr, thresholds = roc_curve(label, out[:, 1])\n",
    "        # auc_rf = auc(fpr, tpr)\n",
    "        # plt.figure(1)\n",
    "        # plt.plot([0, 1], [0, 1], 'k--')\n",
    "        # plt.plot(fpr, tpr, label='Vit (area = {:.3f})'.format(auc_rf))\n",
    "        # plt.xlabel('False positive rate')\n",
    "        # plt.ylabel('True positive rate')\n",
    "        # plt.title('ROC curve')\n",
    "        # plt.legend(loc='best')\n",
    "        # self.logger.experiment.add_figure('AUC Curve', plt.gcf(), self.current_epoch)\n",
    "\n",
    "        return { 'loss': loss.item(), 'preds': out, 'target': label}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        preds = torch.cat([tmp['preds'] for tmp in outputs])\n",
    "        targets = torch.cat([tmp['target'] for tmp in outputs])\n",
    "        confusion_matrix = torchmetrics.functional.confusion_matrix(preds, targets, num_classes=args.num_classes)\n",
    "\n",
    "        df_cm = pd.DataFrame(confusion_matrix.cpu().numpy(), index = range(args.num_classes), columns=range(args.num_classes))\n",
    "        plt.figure(figsize = (args.num_classes,args.num_classes*2))\n",
    "        fig_ = sns.heatmap(df_cm, annot=True, cmap='Spectral').get_figure()\n",
    "        plt.close(fig_)\n",
    "        \n",
    "        #self.logger.experiment.add_figure(\"Confusion matrix\", fig_, self.current_epoch)\n",
    "\n",
    "        # fpr, tpr, thresholds = roc_curve(targets, preds[:, 1])\n",
    "        # auc_rf = auc(fpr, tpr)\n",
    "        # plt.figure(1)\n",
    "        # plt.plot([0, 1], [0, 1], 'k--')\n",
    "        # plt.plot(fpr, tpr, label='Vit (area = {:.3f})'.format(auc_rf))\n",
    "        # plt.xlabel('False positive rate')\n",
    "        # plt.ylabel('True positive rate')\n",
    "        # plt.title('ROC/AUC curve')\n",
    "        # plt.legend(loc='best')\n",
    "        # self.logger.experiment.add_figure('ROC/AUC Curve', plt.gcf(), self.current_epoch)\n",
    "\n",
    "        # repo_root = os.path.abspath(os.getcwd())\n",
    "        # data_root = os.path.join(repo_root, \"logs\")\n",
    "        # list_of_files = glob.glob(f'{data_root}/*') # * means all if need specific format then *.csv\n",
    "        # latest_file = max(list_of_files, key=os.path.getctime)\n",
    "        # writer = SummaryWriter(latest_file)\n",
    "        # writer.add_figure(\"Confusion matrix\", fig_, self.current_epoch)\n",
    "\n",
    "    # def _log_image(self, image):\n",
    "    #     grid = torchvision.utils.make_grid(image, nrow=4)\n",
    "    #     self.logger.experiment.log_image(grid.permute(1,2,0))\n",
    "    #     print(\"[INFO] LOG IMAGE!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = get_experiment_name(args)\n",
    "if args.api_key:\n",
    "    print(\"[INFO] Log with Comet.ml!\")\n",
    "    logger = pl.loggers.CometLogger(\n",
    "        api_key=args.api_key,\n",
    "        save_dir=\"logs\",\n",
    "        project_name=args.project_name,\n",
    "        experiment_name=experiment_name\n",
    "    )\n",
    "    refresh_rate = 0\n",
    "else:\n",
    "    print(\"[INFO] Log with CSV\")\n",
    "    logger = pl.loggers.CSVLogger(\n",
    "        save_dir=\"logs\",\n",
    "        name=experiment_name\n",
    "    )\n",
    "    #logger = TensorBoardLogger(name=\"vit_siim\",save_dir=\"logs\")\n",
    "    refresh_rate = 1\n",
    "args.model_name = 'vit'\n",
    "args.experiment_name = 'vit_siim'    \n",
    "net = Net(args)\n",
    "trainer = pl.Trainer(precision=args.precision,fast_dev_run=args.dry_run, gpus=args.gpus, benchmark=args.benchmark,logger=logger, max_epochs=args.max_epochs)\n",
    "trainer.fit(model=net, train_dataloader=train_dl, val_dataloaders=test_dl)\n",
    "if not args.dry_run:\n",
    "    model_path = f\"weights/{experiment_name}.pth\"\n",
    "    torch.save(net.state_dict(), model_path)\n",
    "    if args.api_key:\n",
    "        logger.experiment.log_asset(file_name=experiment_name, file_data=model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from siim import SIIM\n",
    "import torchvision.transforms as transforms\n",
    "df_test = pd.read_csv(os.path.join('D:/Workspace/cv_attention/data/siim', 'test.csv'))\n",
    "df_test['filepath'] = df_test['image_name'].apply(lambda x: os.path.join('data/siim', 'test', f'{x}.jpg'))\n",
    "n_test = 8\n",
    "# repo_root = os.path.abspath(os.getcwd())\n",
    "# data_root = os.path.join(repo_root, \"data/siim\")\n",
    "data_root = \"D:/Workspace/cv_attention/data/siim\"\n",
    "test_transform = []\n",
    "if args.dataset == 'siim': \n",
    "        test_transform += [transforms.Resize(size=(32, 32))]\n",
    "test_transform = transforms.Compose(test_transform)\n",
    "\n",
    "test_ds = SIIM(data_root, purpose='test', seed=args.seed, split=0.7, transforms=test_transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_ds, batch_size=args.batch_size, num_workers=args.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "OUTPUTS = []\n",
    "\n",
    "\n",
    "model = net\n",
    "model.load_state_dict(torch.load(os.path.join('weights/vit_siim.pth')), strict=True)\n",
    "model.eval()\n",
    "\n",
    "LOGITS = []\n",
    "PROBS = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for (data) in tqdm(test_loader):         \n",
    "        data = data#.to(device)\n",
    "        logits = torch.zeros((data.shape[0], 2))#.to(device)\n",
    "        probs = torch.zeros((data.shape[0], 2))#.to(device)\n",
    "        for I in range(n_test):\n",
    "            l = model(data)\n",
    "            logits += l\n",
    "            probs += l.softmax(1)\n",
    "        logits /= n_test\n",
    "        probs /= n_test\n",
    "\n",
    "        LOGITS.append(logits.detach().cpu())\n",
    "        PROBS.append(probs.detach().cpu())\n",
    "\n",
    "LOGITS = torch.cat(LOGITS).numpy()\n",
    "PROBS = torch.cat(PROBS).numpy()\n",
    "\n",
    "OUTPUTS.append(PROBS[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.zeros(OUTPUTS[0].shape[0])\n",
    "for probs in OUTPUTS:\n",
    "    pred += pd.Series(probs).rank(pct=True).values\n",
    "pred /= len(OUTPUTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test['target'] = pred\n",
    "# df_test[['image_name', 'target']].to_csv(f'submission.csv', index=False)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = get_experiment_name(args)\n",
    "if args.api_key:\n",
    "    print(\"[INFO] Log with Comet.ml!\")\n",
    "    logger = pl.loggers.CometLogger(\n",
    "        api_key=args.api_key,\n",
    "        save_dir=\"logs\",\n",
    "        project_name=args.project_name,\n",
    "        experiment_name=experiment_name\n",
    "    )\n",
    "    refresh_rate = 0\n",
    "else:\n",
    "    print(\"[INFO] Log with CSV\")\n",
    "    logger = pl.loggers.CSVLogger(\n",
    "        save_dir=\"logs\",\n",
    "        name=experiment_name\n",
    "    )\n",
    "    #logger = TensorBoardLogger(name=\"vit_siim\",save_dir=\"logs\")\n",
    "    refresh_rate = 1\n",
    "args.model_name = 'cnn'\n",
    "args.experiment_name = 'cnn_siim'\n",
    "net = Net(args)\n",
    "trainer = pl.Trainer(precision=args.precision,fast_dev_run=args.dry_run, gpus=args.gpus, benchmark=args.benchmark,logger=logger, max_epochs=args.max_epochs)\n",
    "trainer.fit(model=net, train_dataloader=train_dl, val_dataloaders=test_dl)\n",
    "if not args.dry_run:\n",
    "    model_path = f\"weights/{args.experiment_name}.pth\"\n",
    "    torch.save(net.state_dict(), model_path)\n",
    "    if args.api_key:\n",
    "        logger.experiment.log_asset(file_name=args.experiment_name, file_data=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = get_experiment_name(args)\n",
    "if args.api_key:\n",
    "    print(\"[INFO] Log with Comet.ml!\")\n",
    "    logger = pl.loggers.CometLogger(\n",
    "        api_key=args.api_key,\n",
    "        save_dir=\"logs\",\n",
    "        project_name=args.project_name,\n",
    "        experiment_name=experiment_name\n",
    "    )\n",
    "    refresh_rate = 0\n",
    "else:\n",
    "    print(\"[INFO] Log with CSV\")\n",
    "    logger = pl.loggers.CSVLogger(\n",
    "        save_dir=\"logs\",\n",
    "        name=experiment_name\n",
    "    )\n",
    "    #logger = TensorBoardLogger(name=\"vit_siim\",save_dir=\"logs\")\n",
    "    refresh_rate = 1\n",
    "args.model_name = 'vit_emb'\n",
    "args.experiment_name = 'vit_emb_siim'\n",
    "net = Net(args)\n",
    "trainer = pl.Trainer(precision=args.precision,fast_dev_run=args.dry_run, gpus=args.gpus, benchmark=args.benchmark,logger=logger, max_epochs=args.max_epochs)\n",
    "trainer.fit(model=net, train_dataloader=train_dl, val_dataloaders=test_dl)\n",
    "if not args.dry_run:\n",
    "    model_path = f\"weights/{args.experiment_name}.pth\"\n",
    "    torch.save(net.state_dict(), model_path)\n",
    "    if args.api_key:\n",
    "        logger.experiment.log_asset(file_name=args.experiment_name, file_data=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Net(args)\n",
    "model.load_state_dict(torch.load(os.path.join('weights/vit_siim.pth')), strict=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from vit_embedded import ViTEmbedded\n",
    "#newmodel = model.model.feature_extractor\n",
    "ex = ViTEmbedded(64, \n",
    "            2, \n",
    "            img_size=8, \n",
    "            patch=8, \n",
    "            dropout=args.dropout, \n",
    "            mlp_hidden=args.mlp_hidden,\n",
    "            num_layers=args.num_layers,\n",
    "            hidden=args.hidden,\n",
    "            head=args.head,\n",
    "            is_cls_token=args.is_cls_token)\n",
    "\n",
    "dl = next(iter(train_dl))\n",
    "\n",
    "ex.forward(dl[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torchvision.models as models\n",
    "res = models.resnet18(pretrained=False)\n",
    "res = nn.Sequential(*list(res.children())[:-4])\n",
    "d = res.forward(dl[0])\n",
    "d.shape\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "#mo = models.resnet18(pretrained=False)\n",
    "model.model.classifier = model.model.feature_extractor\n",
    "print(model.model.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.attention.CoAtNet import CoAtNet\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "input=torch.randn(1,3,224,224)\n",
    "mbconv=CoAtNet(in_ch=3,image_size=224)\n",
    "out=mbconv(input)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "#import torchvision\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "class HAM(Dataset):\n",
    "    \"\"\"HAM dataset class\"\"\"\n",
    "\n",
    "    def __init__(self, root, purpose, seed, split, transforms=None, tfm_on_patch=None):\n",
    "        self.root_path = root\n",
    "        self.purpose = purpose\n",
    "        self.seed = seed\n",
    "        self.split = split\n",
    "        self.img_part1 = os.listdir(f'{root}/HAM10000_images_part_1/')\n",
    "        self.img_part2 = os.listdir(f'{root}/HAM10000_images_part_2/')\n",
    "        self.images, self.labels = self._make_dataset(directory=self.root_path, purpose=self.purpose, seed=self.seed, split=self.split)\n",
    "        self.transforms = transforms\n",
    "        self.tfm_on_patch = tfm_on_patch\n",
    "\n",
    "    def _make_dataset(self,directory, purpose, seed, split):\n",
    "        \"\"\"\n",
    "        Create the image dataset by preparing a list of samples\n",
    "        :param directory: root directory of the dataset\n",
    "        :returns: (images, labels) where:\n",
    "            - images is a numpy array containing all images in the dataset\n",
    "            - labels is a list containing one label per image\n",
    "        \"\"\"\n",
    "\n",
    "        data_path = os.path.join(directory, \"HAM10000_metadata.csv\")\n",
    "        meta_df = pd.read_csv(data_path)\n",
    "        # meta_df.rename(columns={'image_id': 'image_name'})\n",
    "        meta_df['target'] = pd.Categorical(meta_df['dx']).codes\n",
    "        meta_df['image_name'] = meta_df.apply(lambda row: self.extract_path_img(directory,row.image_id), axis=1)\n",
    "\n",
    "        \n",
    "    \n",
    "        #(33126, 8)\n",
    "        \n",
    "        train, val = train_test_split(meta_df, test_size=split, random_state=seed)\n",
    "        #do we want to apply stratification here?\n",
    "        # train, val, test = np.split(meta_df.sample(frac=1, random_state=seed), \n",
    "        #                                 [int(split*meta_df.shape[0]), int(((1.0-split)/2.0+split)*meta_df.shape[0])])\n",
    "\n",
    "        #train -> 24844\n",
    "        #val -> 8282\n",
    "        # trueRows = train[train['target'] == 1] # 434\n",
    "        # falseRows = train[train['target'] == 0] # 24410\n",
    "        # # # print(len(trueRows))\n",
    "        # # # print(f\" = > {len(falseRows) - len(trueRows)}\")\n",
    "        # trueReplicas = pd.concat([trueRows]*(math.ceil(len(falseRows)/len(trueRows)))) # 434*57 = 24738\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        # oversampled = falseRows.append(trueReplicas[:len(falseRows) - len(trueRows)], ignore_index=True) # 24410 + 23976  = 48386\n",
    "        ######################\n",
    "\n",
    "        if purpose=='train':\n",
    "            return train['image_name'].tolist(), train['target'].tolist()\n",
    "        elif purpose=='val':\n",
    "            return val['image_name'].tolist(), val['target'].tolist()\n",
    "        elif purpose=='test':\n",
    "            data_path = os.path.join(directory, \"test.csv\")\n",
    "            test_df = pd.read_csv(data_path, sep=',')\n",
    "\n",
    "            return test_df['image_name'].tolist(), []\n",
    "\n",
    "    def extract_path_img(self,directory,x):\n",
    "        file = x + '.jpg'\n",
    "        \n",
    "        if file in self.img_part1:\n",
    "            \n",
    "            return os.path.join(f'{directory}/HAM10000_images_part_1', file)\n",
    "        \n",
    "        elif file in self.img_part2:\n",
    "            \n",
    "            return os.path.join(f'{directory}/HAM10000_images_part_2', file)\n",
    "    def __len__(self):\n",
    "        \"\"\"Return number of images in the dataset\"\"\"\n",
    "        return(len(self.images))\n",
    "        \n",
    "    def get_labels(self): return self.labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Creates a dict of the data at the given index:\n",
    "            {\"image\": <i-th image>,                                              #\n",
    "             \"label\": <label of i-th image>} \n",
    "        \"\"\"\n",
    "\n",
    "        img_root = self.images[index]\n",
    "        \n",
    "        #img = Image.open(img_root)\n",
    "        #trans = transforms.ToTensor()\n",
    "        #img = trans(img)\n",
    "        #img = torchvision.io.read_image(img_root)\n",
    "        img = read_image(img_root)\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "\n",
    "        if self.labels[index] == 1:\n",
    "            transformForReplicas = transforms.RandomChoice([\n",
    "                transforms.RandomHorizontalFlip(), \n",
    "                transforms.RandomVerticalFlip(),\n",
    "                transforms.RandomAutocontrast(),\n",
    "                transforms.RandomAdjustSharpness(sharpness_factor=2),\n",
    "                transforms.RandomEqualize()\n",
    "            ])\n",
    "\n",
    "            transformForReplicas2 = transforms.RandomChoice([\n",
    "                transforms.ColorJitter(brightness=.5, hue=.3),\n",
    "                transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n",
    "                transforms.RandomRotation(degrees=(0, 180)),\n",
    "                transforms.RandomPosterize(bits=2),\n",
    "                \n",
    "            ])\n",
    "\n",
    "            img = transformForReplicas(img)\n",
    "            img = transformForReplicas2(img)\n",
    "        img = img.float()\n",
    "\n",
    "        \n",
    "        if self.purpose == 'test':\n",
    "            return img\n",
    "        else:\n",
    "            return img, self.labels[index]\n",
    "        return img, torch.tensor([self.labels[index]])\n",
    "\n",
    "\n",
    "        data_dict = {\n",
    "            'image': img,\n",
    "            'label': torch.tensor([self.labels[index]])\n",
    "        }\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        # if self.tfm_on_patch is None: return data_dict\n",
    "\n",
    "        # for tfm in self.tfm_on_patch:\n",
    "        #     data_dict = tfm(data_dict)\n",
    "\n",
    "        \n",
    "\n",
    "        return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HAM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/k.urbanczyk/Documents/datascience/datapractical/vit-siim/play.ipynb Cell 2'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/k.urbanczyk/Documents/datascience/datapractical/vit-siim/play.ipynb#ch0000027?line=0'>1</a>\u001b[0m base_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/dss/dssmcmlfs01/pn69za/pn69za-dss-0002/ra49tad2/data/ham\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/k.urbanczyk/Documents/datascience/datapractical/vit-siim/play.ipynb#ch0000027?line=1'>2</a>\u001b[0m train_ds \u001b[39m=\u001b[39m HAM(base_path, purpose\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m, seed\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m, split\u001b[39m=\u001b[39m\u001b[39m0.25\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'HAM' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "base_path = \"/dss/dssmcmlfs01/pn69za/pn69za-dss-0002/ra49tad2/data/ham\"\n",
    "train_ds = HAM(base_path, purpose='train', seed=42, split=0.25)\n",
    "\n",
    "\n",
    "\n",
    "# train, val = train_test_split(meta_df, test_size=split, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df['cancer_idx'] = pd.Categorical(meta_df['dx']).codes\n",
    "meta_df['image'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_part1 = os.listdir(f'{base_path}/HAM10000_images_part_1/')\n",
    "img_part2 = os.listdir(f'{base_path}/HAM10000_images_part_2/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('vit')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2cf8d89f9befaf1d28407f455a298e72210d4682f6824944c8efae7526d4df06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
