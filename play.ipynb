{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import argparse\n",
    "from sklearn import metrics\n",
    "import comet_ml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import pytorch_lightning as pl\n",
    "import warmup_scheduler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "import torchmetrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from utils import get_model, get_dataset, get_experiment_name, get_criterion\n",
    "from da import CutMix, MixUp\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "class Settings:\n",
    "    def __init__(self):\n",
    "        self.dataset = \"siim\"\n",
    "        self.num_classes = 2\n",
    "        self.model_name = \"vit\"\n",
    "        self.patch = 8\n",
    "        self.batch_size = 8\n",
    "        self.eval_batch_size = 1024\n",
    "        self.lr = 1e-3\n",
    "        self.min_lr = 1e-5\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.999\n",
    "        self.max_epochs = 3\n",
    "        self.weight_decay = 5e-5\n",
    "        self.warmup_epoch = 5\n",
    "        self.precision = 16\n",
    "        self.criterion = \"ce\"\n",
    "        self.smoothing = 0.1\n",
    "        self.dropout = 0.0\n",
    "        self.head = 12\n",
    "        self.num_layers = 7\n",
    "        self.hidden = 384\n",
    "        self.label_smoothing = False\n",
    "        self.mlp_hidden = 384\n",
    "        self.seed = 42\n",
    "        self.project_name = \"VisionTransformer\"\n",
    "        self.off_benchmark = False\n",
    "        self.dry_run = False\n",
    "        self.autoaugment = False\n",
    "        self.rcpaste = False\n",
    "        self.cutmix = False\n",
    "        self.mixup = False\n",
    "        self.off_cls_token = False\n",
    "        self.api_key = False\n",
    "\n",
    "args = Settings()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "args.benchmark = True if not args.off_benchmark else False\n",
    "args.gpus = torch.cuda.device_count()\n",
    "args.num_workers = 4*args.gpus if args.gpus else 8\n",
    "args.is_cls_token = True if not args.off_cls_token else False\n",
    "if not args.gpus:\n",
    "    args.precision=32\n",
    "\n",
    "if args.mlp_hidden != args.hidden*4:\n",
    "    print(f\"[INFO] In original paper, mlp_hidden(CURRENT:{args.mlp_hidden}) is set to: {args.hidden*4}(={args.hidden}*4)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import argparse\n",
    "from sklearn import metrics\n",
    "#import comet_ml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import pytorch_lightning as pl\n",
    "import warmup_scheduler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "import torchmetrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from utils import get_model, get_dataset, get_experiment_name, get_criterion\n",
    "from da import CutMix, MixUp\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from ham import HAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "#import torchvision\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class HAM(Dataset):\n",
    "    \"\"\"HAM dataset class\"\"\"\n",
    "\n",
    "    def __init__(self, root, purpose, seed, split, transforms=None, tfm_on_patch=None):\n",
    "        self.root_path = root\n",
    "        self.purpose = purpose\n",
    "        self.seed = seed\n",
    "        self.split = split\n",
    "        self.img_part1 = os.listdir(f'{root}/HAM10000_images_part_1/')\n",
    "        self.img_part2 = os.listdir(f'{root}/HAM10000_images_part_2/')\n",
    "        self.images, self.labels = self._make_dataset(directory=self.root_path, purpose=self.purpose, seed=self.seed, split=self.split)\n",
    "        self.transforms = transforms\n",
    "        self.tfm_on_patch = tfm_on_patch\n",
    "\n",
    "    def _make_dataset(self,directory, purpose, seed, split):\n",
    "        \"\"\"\n",
    "        Create the image dataset by preparing a list of samples\n",
    "        :param directory: root directory of the dataset\n",
    "        :returns: (images, labels) where:\n",
    "            - images is a numpy array containing all images in the dataset\n",
    "            - labels is a list containing one label per image\n",
    "        \"\"\"\n",
    "\n",
    "        data_path = os.path.join(directory, \"HAM10000_metadata.csv\")\n",
    "        meta_df = pd.read_csv(data_path)\n",
    "        # meta_df.rename(columns={'image_id': 'image_name'})\n",
    "        meta_df['target'] = pd.Categorical(meta_df['dx']).codes\n",
    "        no_of_classes = meta_df['target'].unique()\n",
    "        print(f'No. of Class in HAM: {no_of_classes}')\n",
    "        meta_df['image_name'] = meta_df.apply(lambda row: self.extract_path_img(directory,row.image_id), axis=1)\n",
    "\n",
    "    \n",
    "        #(33126, 8)\n",
    "        \n",
    "        train, val = train_test_split(meta_df, test_size=split, random_state=seed)\n",
    "        #do we want to apply stratification here?\n",
    "        # train, val, test = np.split(meta_df.sample(frac=1, random_state=seed), \n",
    "        #                                 [int(split*meta_df.shape[0]), int(((1.0-split)/2.0+split)*meta_df.shape[0])])\n",
    "\n",
    "        #train -> 24844\n",
    "        #val -> 8282\n",
    "        # trueRows = train[train['target'] == 1] # 434\n",
    "        # falseRows = train[train['target'] == 0] # 24410\n",
    "        # # # print(len(trueRows))\n",
    "        # # # print(f\" = > {len(falseRows) - len(trueRows)}\")\n",
    "        # trueReplicas = pd.concat([trueRows]*(math.ceil(len(falseRows)/len(trueRows)))) # 434*57 = 24738\n",
    "\n",
    "        print(train['target'])\n",
    "        \n",
    "        \n",
    "        # oversampled = falseRows.append(trueReplicas[:len(falseRows) - len(trueRows)], ignore_index=True) # 24410 + 23976  = 48386\n",
    "        ######################\n",
    "\n",
    "        if purpose=='train':\n",
    "            return train['image_name'].tolist(), train['target'].tolist()\n",
    "        elif purpose=='val':\n",
    "            return val['image_name'].tolist(), val['target'].tolist()\n",
    "        elif purpose=='test':\n",
    "            data_path = os.path.join(directory, \"test.csv\")\n",
    "            test_df = pd.read_csv(data_path, sep=',')\n",
    "\n",
    "            return test_df['image_name'].tolist(), []\n",
    "\n",
    "    def extract_path_img(self,directory,x):\n",
    "        file = x + '.jpg'\n",
    "        \n",
    "        if file in self.img_part1:\n",
    "            \n",
    "            return os.path.join(f'{directory}/HAM10000_images_part_1', file)\n",
    "        \n",
    "        elif file in self.img_part2:\n",
    "            \n",
    "            return os.path.join(f'{directory}/HAM10000_images_part_2', file)\n",
    "    def __len__(self):\n",
    "        \"\"\"Return number of images in the dataset\"\"\"\n",
    "        return(len(self.images))\n",
    "        \n",
    "    def get_labels(self): return self.labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Creates a dict of the data at the given index:\n",
    "            {\"image\": <i-th image>,                                              #\n",
    "             \"label\": <label of i-th image>} \n",
    "        \"\"\"\n",
    "        img_root = self.images[index]\n",
    "        \n",
    "        #img = Image.open(img_root)\n",
    "        #trans = transforms.ToTensor()\n",
    "        #img = trans(img)\n",
    "        #img = torchvision.io.read_image(img_root)\n",
    "        img = read_image(img_root)\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "\n",
    "        if self.labels[index] == 1:\n",
    "            transformForReplicas = transforms.RandomChoice([\n",
    "                transforms.RandomHorizontalFlip(), \n",
    "                transforms.RandomVerticalFlip(),\n",
    "                transforms.RandomAutocontrast(),\n",
    "                transforms.RandomAdjustSharpness(sharpness_factor=2),\n",
    "                transforms.RandomEqualize()\n",
    "            ])\n",
    "\n",
    "            transformForReplicas2 = transforms.RandomChoice([\n",
    "                transforms.ColorJitter(brightness=.5, hue=.3),\n",
    "                transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n",
    "                transforms.RandomRotation(degrees=(0, 180)),\n",
    "                transforms.RandomPosterize(bits=2),\n",
    "                \n",
    "            ])\n",
    "\n",
    "            img = transformForReplicas(img)\n",
    "            img = transformForReplicas2(img)\n",
    "        img = img.float()\n",
    "     \n",
    "        if self.purpose == 'test':\n",
    "            return img\n",
    "        else:\n",
    "            return img, self.labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_path_img(directory,x):\n",
    "    file = x + '.jpg'\n",
    "        \n",
    "    if file in os.listdir(f'{directory}/HAM10000_images_part_1/'):\n",
    "            \n",
    "        return os.path.join(f'{directory}/HAM10000_images_part_1', file)\n",
    "        \n",
    "    elif file in os.listdir(f'{directory}/HAM10000_images_part_2/'):\n",
    "            \n",
    "        return os.path.join(f'{directory}/HAM10000_images_part_2', file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Class in HAM: [2 5 3 4 6 1 0]\n",
      "4219    5\n",
      "9092    5\n",
      "8016    5\n",
      "3292    5\n",
      "819     2\n",
      "       ..\n",
      "5734    5\n",
      "5191    5\n",
      "5390    5\n",
      "860     2\n",
      "7270    5\n",
      "Name: target, Length: 2003, dtype: int8\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "base_path = \"/Users/k.urbanczyk/Desktop/archive\"\n",
    "#train_ds = HAM(base_path, purpose='train', seed=42, split=0.25)\n",
    "\n",
    "\n",
    "data_path = os.path.join(base_path, \"HAM10000_metadata.csv\")\n",
    "meta_df = pd.read_csv(data_path)\n",
    "# meta_df.rename(columns={'image_id': 'image_name'})\n",
    "meta_df['target'] = pd.Categorical(meta_df['dx']).codes\n",
    "no_of_classes = meta_df['target'].unique()\n",
    "print(f'No. of Class in HAM: {no_of_classes}')\n",
    "meta_df['image_name'] = meta_df.apply(lambda row: extract_path_img(base_path, row.image_id), axis=1)\n",
    "\n",
    "    \n",
    "#(33126, 8)\n",
    "        \n",
    "train, val = train_test_split(meta_df, test_size=0.8, random_state=42)\n",
    "print(train['target'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "most_popular_count = train['target'].value_counts().max()\n",
    "most_popular_idx = train['target'].value_counts().idxmax()\n",
    "oversampled = train[train['target'] == most_popular_idx]\n",
    "print(most_popular_idx)\n",
    "for i in range(0,7):\n",
    "    if most_popular_idx != i:\n",
    "        to_be_replicated = train[train['target'] == i]\n",
    "        replicas = pd.concat([trueRows]*(math.ceil(len(falseRows)/len(trueRows))))\n",
    "    #trueReplicas = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "\n",
    "\n",
    "class ImbalancedDatasetSampler(torch.utils.data.sampler.Sampler):\n",
    "    \"\"\"Samples elements randomly from a given list of indices for imbalanced dataset\n",
    "    Arguments:\n",
    "        indices (list, optional): a list of indices\n",
    "        num_samples (int, optional): number of samples to draw\n",
    "        callback_get_label func: a callback-like function which takes two arguments - dataset and index\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, indices=None, num_samples=None, callback_get_label=None):\n",
    "                \n",
    "        # if indices is not provided, \n",
    "        # all elements in the dataset will be considered\n",
    "        self.indices = list(range(len(dataset))) \\\n",
    "            if indices is None else indices\n",
    "\n",
    "        # define custom callback\n",
    "        self.callback_get_label = callback_get_label\n",
    "\n",
    "        # if num_samples is not provided, \n",
    "        # draw `len(indices)` samples in each iteration\n",
    "        self.num_samples = len(self.indices) \\\n",
    "            if num_samples is None else num_samples\n",
    "            \n",
    "        # distribution of classes in the dataset \n",
    "        label_to_count = {}\n",
    "        for idx in self.indices:\n",
    "            label = self._get_label(dataset, idx)\n",
    "            if label in label_to_count:\n",
    "                label_to_count[label] += 1\n",
    "            else:\n",
    "                label_to_count[label] = 1\n",
    "                \n",
    "        # weight for each sample\n",
    "        weights = [1.0 / label_to_count[self._get_label(dataset, idx)]\n",
    "                   for idx in self.indices]\n",
    "        self.weights = torch.DoubleTensor(weights)\n",
    "\n",
    "    def _get_label(self, dataset, idx):\n",
    "        return dataset.train_labels[idx].item()\n",
    "                \n",
    "    def __iter__(self):\n",
    "        return (self.indices[i] for i in torch.multinomial(\n",
    "            self.weights, self.num_samples, replacement=True))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, test_ds = get_dataset(args)\n",
    "# classes = torch.tensor([0, 1, 2])\n",
    "# indices = (torch.tensor(train_ds.targets)[..., None] == classes).any(-1).nonzero(as_tuple=True)[0]\n",
    "# train_ds = torch.utils.data.Subset(train_ds, indices)\n",
    "# test_ds = torch.utils.data.Subset(test_ds, indices)\n",
    "#print(data.shape)\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=args.batch_size, shuffle=True)#, num_workers=args.num_workers, pin_memory=True)\n",
    "test_dl = torch.utils.data.DataLoader(test_ds, batch_size=args.eval_batch_size, num_workers=args.num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsampler import ImbalancedDatasetSampler\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_ds, \n",
    "    sampler=ImbalancedDatasetSampler(train_ds),\n",
    "    batch_size=args.batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "applier = transforms.RandomApply(transforms=[transforms.RandomCrop(size=(64, 64))], p=0.5)\n",
    "transformed_imgs = applier(train_ds[0][0])\n",
    "plt.imshow((train_ds[4][0].permute(1, 2, 0)).numpy().astype(np.uint8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_it_somehow = train_ds[1][0]\n",
    "\n",
    "# img = plot_it_somehow.swapaxes(0, 1)\n",
    "# img = img.swapaxes(1, 2)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow((plot_it_somehow.permute(1, 2, 0)).numpy().astype(np.uint8))\n",
    "plt.show()\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# pil_image=Image.fromarray(plot_it_somehow.permute(1, 2, 0).numpy().astype(np.uint8))\n",
    "# pil_image.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "# sample input (10 RGB images containing just Gaussian Noise)\n",
    "batch_tensor = ex[0]   # (N, C, H, W)\n",
    "\n",
    "# make grid (2 rows and 5 columns) to display our 10 images\n",
    "grid_img = torchvision.utils.make_grid(batch_tensor, nrow=5)\n",
    "\n",
    "grid_img.shape\n",
    "plt.imshow((grid_img.permute(1, 2, 0)).numpy().astype(np.uint8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super(Net, self).__init__()\n",
    "        # self.hparams = hparams\n",
    "        self.hparams.update(vars(hparams))\n",
    "        self.model = get_model(hparams)\n",
    "        self.criterion = get_criterion(args)\n",
    "        if hparams.cutmix:\n",
    "            self.cutmix = CutMix(hparams.size, beta=1.)\n",
    "        if hparams.mixup:\n",
    "            self.mixup = MixUp(alpha=1.)\n",
    "        self.log_image_flag = hparams.api_key is None\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.hparams.lr, betas=(self.hparams.beta1, self.hparams.beta2), weight_decay=self.hparams.weight_decay)\n",
    "        self.base_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=self.hparams.max_epochs, eta_min=self.hparams.min_lr)\n",
    "        self.scheduler = warmup_scheduler.GradualWarmupScheduler(self.optimizer, multiplier=1., total_epoch=self.hparams.warmup_epoch, after_scheduler=self.base_scheduler)\n",
    "        return [self.optimizer], [self.scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        img, label = batch\n",
    "        if self.hparams.cutmix or self.hparams.mixup:\n",
    "            if self.hparams.cutmix:\n",
    "                img, label, rand_label, lambda_= self.cutmix((img, label))\n",
    "            elif self.hparams.mixup:\n",
    "                if np.random.rand() <= 0.8:\n",
    "                    img, label, rand_label, lambda_ = self.mixup((img, label))\n",
    "                else:\n",
    "                    img, label, rand_label, lambda_ = img, label, torch.zeros_like(label), 1.\n",
    "            out = self.model(img)\n",
    "            loss = self.criterion(out, label)*lambda_ + self.criterion(out, rand_label)*(1.-lambda_)\n",
    "        else:\n",
    "            out = self(img)\n",
    "            loss = self.criterion(out[:,1], label.float())\n",
    "\n",
    "        if not self.log_image_flag and not self.hparams.dry_run:\n",
    "            self.log_image_flag = True\n",
    "            #self._log_image(img.clone().detach().cpu())\n",
    "\n",
    "        acc = torch.eq(out.argmax(-1), label).float().mean()\n",
    "        auc_score = metrics.roc_auc_score(label, out[:, 1].squeeze().detach().numpy())\n",
    "        self.log('auc', auc_score, on_step=True, on_epoch=True)\n",
    "        self.log('acc', acc, on_step=True, on_epoch=True)\n",
    "        self.log('loss', loss,on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        #self.log(\"lr\", self.optimizer.param_groups[0][\"lr\"], on_epoch=self.current_epoch)\n",
    "        self.log(\"lr\", self.optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        img, label = batch\n",
    "        out = self(img)\n",
    "        loss = self.criterion(out[:,1], label.float())\n",
    "        acc = torch.eq(out.argmax(-1), label).float().mean()\n",
    "        #self.log(\"val_loss\", loss)\n",
    "        #self.log(\"val_acc\", acc)\n",
    "\n",
    "        auc_score = metrics.roc_auc_score(label.cpu(), out[:, 1].squeeze())\n",
    "        self.log('auc', auc_score, on_step=True, on_epoch=True)\n",
    "        val_acc = torchmetrics.functional.accuracy(out[:, 1], label)\n",
    "        self.log('valid_acc_from_tmet', val_acc, on_step=True, on_epoch=True)\n",
    "        self.log('valid_acc', acc, on_step=True, on_epoch=True)\n",
    "        self.log('val_loss', loss,on_step=True, on_epoch=True)\n",
    "\n",
    "        # fpr, tpr, thresholds = roc_curve(label, out[:, 1])\n",
    "        # auc_rf = auc(fpr, tpr)\n",
    "        # plt.figure(1)\n",
    "        # plt.plot([0, 1], [0, 1], 'k--')\n",
    "        # plt.plot(fpr, tpr, label='Vit (area = {:.3f})'.format(auc_rf))\n",
    "        # plt.xlabel('False positive rate')\n",
    "        # plt.ylabel('True positive rate')\n",
    "        # plt.title('ROC curve')\n",
    "        # plt.legend(loc='best')\n",
    "        # self.logger.experiment.add_figure('AUC Curve', plt.gcf(), self.current_epoch)\n",
    "\n",
    "        return { 'loss': loss.item(), 'preds': out, 'target': label}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        preds = torch.cat([tmp['preds'] for tmp in outputs])\n",
    "        targets = torch.cat([tmp['target'] for tmp in outputs])\n",
    "        confusion_matrix = torchmetrics.functional.confusion_matrix(preds, targets, num_classes=args.num_classes)\n",
    "\n",
    "        df_cm = pd.DataFrame(confusion_matrix.cpu().numpy(), index = range(args.num_classes), columns=range(args.num_classes))\n",
    "        plt.figure(figsize = (args.num_classes,args.num_classes*2))\n",
    "        fig_ = sns.heatmap(df_cm, annot=True, cmap='Spectral').get_figure()\n",
    "        plt.close(fig_)\n",
    "        \n",
    "        #self.logger.experiment.add_figure(\"Confusion matrix\", fig_, self.current_epoch)\n",
    "\n",
    "        # fpr, tpr, thresholds = roc_curve(targets, preds[:, 1])\n",
    "        # auc_rf = auc(fpr, tpr)\n",
    "        # plt.figure(1)\n",
    "        # plt.plot([0, 1], [0, 1], 'k--')\n",
    "        # plt.plot(fpr, tpr, label='Vit (area = {:.3f})'.format(auc_rf))\n",
    "        # plt.xlabel('False positive rate')\n",
    "        # plt.ylabel('True positive rate')\n",
    "        # plt.title('ROC/AUC curve')\n",
    "        # plt.legend(loc='best')\n",
    "        # self.logger.experiment.add_figure('ROC/AUC Curve', plt.gcf(), self.current_epoch)\n",
    "\n",
    "        # repo_root = os.path.abspath(os.getcwd())\n",
    "        # data_root = os.path.join(repo_root, \"logs\")\n",
    "        # list_of_files = glob.glob(f'{data_root}/*') # * means all if need specific format then *.csv\n",
    "        # latest_file = max(list_of_files, key=os.path.getctime)\n",
    "        # writer = SummaryWriter(latest_file)\n",
    "        # writer.add_figure(\"Confusion matrix\", fig_, self.current_epoch)\n",
    "\n",
    "    # def _log_image(self, image):\n",
    "    #     grid = torchvision.utils.make_grid(image, nrow=4)\n",
    "    #     self.logger.experiment.log_image(grid.permute(1,2,0))\n",
    "    #     print(\"[INFO] LOG IMAGE!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = get_experiment_name(args)\n",
    "if args.api_key:\n",
    "    print(\"[INFO] Log with Comet.ml!\")\n",
    "    logger = pl.loggers.CometLogger(\n",
    "        api_key=args.api_key,\n",
    "        save_dir=\"logs\",\n",
    "        project_name=args.project_name,\n",
    "        experiment_name=experiment_name\n",
    "    )\n",
    "    refresh_rate = 0\n",
    "else:\n",
    "    print(\"[INFO] Log with CSV\")\n",
    "    logger = pl.loggers.CSVLogger(\n",
    "        save_dir=\"logs\",\n",
    "        name=experiment_name\n",
    "    )\n",
    "    #logger = TensorBoardLogger(name=\"vit_siim\",save_dir=\"logs\")\n",
    "    refresh_rate = 1\n",
    "args.model_name = 'vit'\n",
    "args.experiment_name = 'vit_siim'    \n",
    "net = Net(args)\n",
    "trainer = pl.Trainer(precision=args.precision,fast_dev_run=args.dry_run, gpus=args.gpus, benchmark=args.benchmark,logger=logger, max_epochs=args.max_epochs)\n",
    "trainer.fit(model=net, train_dataloader=train_dl, val_dataloaders=test_dl)\n",
    "if not args.dry_run:\n",
    "    model_path = f\"weights/{experiment_name}.pth\"\n",
    "    torch.save(net.state_dict(), model_path)\n",
    "    if args.api_key:\n",
    "        logger.experiment.log_asset(file_name=experiment_name, file_data=model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from siim import SIIM\n",
    "import torchvision.transforms as transforms\n",
    "df_test = pd.read_csv(os.path.join('D:/Workspace/cv_attention/data/siim', 'test.csv'))\n",
    "df_test['filepath'] = df_test['image_name'].apply(lambda x: os.path.join('data/siim', 'test', f'{x}.jpg'))\n",
    "n_test = 8\n",
    "# repo_root = os.path.abspath(os.getcwd())\n",
    "# data_root = os.path.join(repo_root, \"data/siim\")\n",
    "data_root = \"D:/Workspace/cv_attention/data/siim\"\n",
    "test_transform = []\n",
    "if args.dataset == 'siim': \n",
    "        test_transform += [transforms.Resize(size=(32, 32))]\n",
    "test_transform = transforms.Compose(test_transform)\n",
    "\n",
    "test_ds = SIIM(data_root, purpose='test', seed=args.seed, split=0.7, transforms=test_transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_ds, batch_size=args.batch_size, num_workers=args.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "OUTPUTS = []\n",
    "\n",
    "\n",
    "model = net\n",
    "model.load_state_dict(torch.load(os.path.join('weights/vit_siim.pth')), strict=True)\n",
    "model.eval()\n",
    "\n",
    "LOGITS = []\n",
    "PROBS = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for (data) in tqdm(test_loader):         \n",
    "        data = data#.to(device)\n",
    "        logits = torch.zeros((data.shape[0], 2))#.to(device)\n",
    "        probs = torch.zeros((data.shape[0], 2))#.to(device)\n",
    "        for I in range(n_test):\n",
    "            l = model(data)\n",
    "            logits += l\n",
    "            probs += l.softmax(1)\n",
    "        logits /= n_test\n",
    "        probs /= n_test\n",
    "\n",
    "        LOGITS.append(logits.detach().cpu())\n",
    "        PROBS.append(probs.detach().cpu())\n",
    "\n",
    "LOGITS = torch.cat(LOGITS).numpy()\n",
    "PROBS = torch.cat(PROBS).numpy()\n",
    "\n",
    "OUTPUTS.append(PROBS[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.zeros(OUTPUTS[0].shape[0])\n",
    "for probs in OUTPUTS:\n",
    "    pred += pd.Series(probs).rank(pct=True).values\n",
    "pred /= len(OUTPUTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test['target'] = pred\n",
    "# df_test[['image_name', 'target']].to_csv(f'submission.csv', index=False)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = get_experiment_name(args)\n",
    "if args.api_key:\n",
    "    print(\"[INFO] Log with Comet.ml!\")\n",
    "    logger = pl.loggers.CometLogger(\n",
    "        api_key=args.api_key,\n",
    "        save_dir=\"logs\",\n",
    "        project_name=args.project_name,\n",
    "        experiment_name=experiment_name\n",
    "    )\n",
    "    refresh_rate = 0\n",
    "else:\n",
    "    print(\"[INFO] Log with CSV\")\n",
    "    logger = pl.loggers.CSVLogger(\n",
    "        save_dir=\"logs\",\n",
    "        name=experiment_name\n",
    "    )\n",
    "    #logger = TensorBoardLogger(name=\"vit_siim\",save_dir=\"logs\")\n",
    "    refresh_rate = 1\n",
    "args.model_name = 'cnn'\n",
    "args.experiment_name = 'cnn_siim'\n",
    "net = Net(args)\n",
    "trainer = pl.Trainer(precision=args.precision,fast_dev_run=args.dry_run, gpus=args.gpus, benchmark=args.benchmark,logger=logger, max_epochs=args.max_epochs)\n",
    "trainer.fit(model=net, train_dataloader=train_dl, val_dataloaders=test_dl)\n",
    "if not args.dry_run:\n",
    "    model_path = f\"weights/{args.experiment_name}.pth\"\n",
    "    torch.save(net.state_dict(), model_path)\n",
    "    if args.api_key:\n",
    "        logger.experiment.log_asset(file_name=args.experiment_name, file_data=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = get_experiment_name(args)\n",
    "if args.api_key:\n",
    "    print(\"[INFO] Log with Comet.ml!\")\n",
    "    logger = pl.loggers.CometLogger(\n",
    "        api_key=args.api_key,\n",
    "        save_dir=\"logs\",\n",
    "        project_name=args.project_name,\n",
    "        experiment_name=experiment_name\n",
    "    )\n",
    "    refresh_rate = 0\n",
    "else:\n",
    "    print(\"[INFO] Log with CSV\")\n",
    "    logger = pl.loggers.CSVLogger(\n",
    "        save_dir=\"logs\",\n",
    "        name=experiment_name\n",
    "    )\n",
    "    #logger = TensorBoardLogger(name=\"vit_siim\",save_dir=\"logs\")\n",
    "    refresh_rate = 1\n",
    "args.model_name = 'vit_emb'\n",
    "args.experiment_name = 'vit_emb_siim'\n",
    "net = Net(args)\n",
    "trainer = pl.Trainer(precision=args.precision,fast_dev_run=args.dry_run, gpus=args.gpus, benchmark=args.benchmark,logger=logger, max_epochs=args.max_epochs)\n",
    "trainer.fit(model=net, train_dataloader=train_dl, val_dataloaders=test_dl)\n",
    "if not args.dry_run:\n",
    "    model_path = f\"weights/{args.experiment_name}.pth\"\n",
    "    torch.save(net.state_dict(), model_path)\n",
    "    if args.api_key:\n",
    "        logger.experiment.log_asset(file_name=args.experiment_name, file_data=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Net(args)\n",
    "model.load_state_dict(torch.load(os.path.join('weights/vit_siim.pth')), strict=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from vit_embedded import ViTEmbedded\n",
    "#newmodel = model.model.feature_extractor\n",
    "ex = ViTEmbedded(64, \n",
    "            2, \n",
    "            img_size=8, \n",
    "            patch=8, \n",
    "            dropout=args.dropout, \n",
    "            mlp_hidden=args.mlp_hidden,\n",
    "            num_layers=args.num_layers,\n",
    "            hidden=args.hidden,\n",
    "            head=args.head,\n",
    "            is_cls_token=args.is_cls_token)\n",
    "\n",
    "dl = next(iter(train_dl))\n",
    "\n",
    "ex.forward(dl[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torchvision.models as models\n",
    "res = models.resnet18(pretrained=False)\n",
    "res = nn.Sequential(*list(res.children())[:-4])\n",
    "d = res.forward(dl[0])\n",
    "d.shape\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "#mo = models.resnet18(pretrained=False)\n",
    "model.model.classifier = model.model.feature_extractor\n",
    "print(model.model.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.attention.CoAtNet import CoAtNet\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "input=torch.randn(1,3,224,224)\n",
    "mbconv=CoAtNet(in_ch=3,image_size=224)\n",
    "out=mbconv(input)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "#import torchvision\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "class HAM(Dataset):\n",
    "    \"\"\"HAM dataset class\"\"\"\n",
    "\n",
    "    def __init__(self, root, purpose, seed, split, transforms=None, tfm_on_patch=None):\n",
    "        self.root_path = root\n",
    "        self.purpose = purpose\n",
    "        self.seed = seed\n",
    "        self.split = split\n",
    "        self.img_part1 = os.listdir(f'{root}/HAM10000_images_part_1/')\n",
    "        self.img_part2 = os.listdir(f'{root}/HAM10000_images_part_2/')\n",
    "        self.images, self.labels = self._make_dataset(directory=self.root_path, purpose=self.purpose, seed=self.seed, split=self.split)\n",
    "        self.transforms = transforms\n",
    "        self.tfm_on_patch = tfm_on_patch\n",
    "\n",
    "    def _make_dataset(self,directory, purpose, seed, split):\n",
    "        \"\"\"\n",
    "        Create the image dataset by preparing a list of samples\n",
    "        :param directory: root directory of the dataset\n",
    "        :returns: (images, labels) where:\n",
    "            - images is a numpy array containing all images in the dataset\n",
    "            - labels is a list containing one label per image\n",
    "        \"\"\"\n",
    "\n",
    "        data_path = os.path.join(directory, \"HAM10000_metadata.csv\")\n",
    "        meta_df = pd.read_csv(data_path)\n",
    "        # meta_df.rename(columns={'image_id': 'image_name'})\n",
    "        meta_df['target'] = pd.Categorical(meta_df['dx']).codes\n",
    "        meta_df['image_name'] = meta_df.apply(lambda row: self.extract_path_img(directory,row.image_id), axis=1)\n",
    "\n",
    "        \n",
    "    \n",
    "        #(33126, 8)\n",
    "        \n",
    "        train, val = train_test_split(meta_df, test_size=split, random_state=seed)\n",
    "        #do we want to apply stratification here?\n",
    "        # train, val, test = np.split(meta_df.sample(frac=1, random_state=seed), \n",
    "        #                                 [int(split*meta_df.shape[0]), int(((1.0-split)/2.0+split)*meta_df.shape[0])])\n",
    "\n",
    "        #train -> 24844\n",
    "        #val -> 8282\n",
    "        # trueRows = train[train['target'] == 1] # 434\n",
    "        # falseRows = train[train['target'] == 0] # 24410\n",
    "        # # # print(len(trueRows))\n",
    "        # # # print(f\" = > {len(falseRows) - len(trueRows)}\")\n",
    "        # trueReplicas = pd.concat([trueRows]*(math.ceil(len(falseRows)/len(trueRows)))) # 434*57 = 24738\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        # oversampled = falseRows.append(trueReplicas[:len(falseRows) - len(trueRows)], ignore_index=True) # 24410 + 23976  = 48386\n",
    "        ######################\n",
    "\n",
    "        if purpose=='train':\n",
    "            return train['image_name'].tolist(), train['target'].tolist()\n",
    "        elif purpose=='val':\n",
    "            return val['image_name'].tolist(), val['target'].tolist()\n",
    "        elif purpose=='test':\n",
    "            data_path = os.path.join(directory, \"test.csv\")\n",
    "            test_df = pd.read_csv(data_path, sep=',')\n",
    "\n",
    "            return test_df['image_name'].tolist(), []\n",
    "\n",
    "    def extract_path_img(self,directory,x):\n",
    "        file = x + '.jpg'\n",
    "        \n",
    "        if file in self.img_part1:\n",
    "            \n",
    "            return os.path.join(f'{directory}/HAM10000_images_part_1', file)\n",
    "        \n",
    "        elif file in self.img_part2:\n",
    "            \n",
    "            return os.path.join(f'{directory}/HAM10000_images_part_2', file)\n",
    "    def __len__(self):\n",
    "        \"\"\"Return number of images in the dataset\"\"\"\n",
    "        return(len(self.images))\n",
    "        \n",
    "    def get_labels(self): return self.labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Creates a dict of the data at the given index:\n",
    "            {\"image\": <i-th image>,                                              #\n",
    "             \"label\": <label of i-th image>} \n",
    "        \"\"\"\n",
    "\n",
    "        img_root = self.images[index]\n",
    "        \n",
    "        #img = Image.open(img_root)\n",
    "        #trans = transforms.ToTensor()\n",
    "        #img = trans(img)\n",
    "        #img = torchvision.io.read_image(img_root)\n",
    "        img = read_image(img_root)\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "\n",
    "        if self.labels[index] == 1:\n",
    "            transformForReplicas = transforms.RandomChoice([\n",
    "                transforms.RandomHorizontalFlip(), \n",
    "                transforms.RandomVerticalFlip(),\n",
    "                transforms.RandomAutocontrast(),\n",
    "                transforms.RandomAdjustSharpness(sharpness_factor=2),\n",
    "                transforms.RandomEqualize()\n",
    "            ])\n",
    "\n",
    "            transformForReplicas2 = transforms.RandomChoice([\n",
    "                transforms.ColorJitter(brightness=.5, hue=.3),\n",
    "                transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n",
    "                transforms.RandomRotation(degrees=(0, 180)),\n",
    "                transforms.RandomPosterize(bits=2),\n",
    "                \n",
    "            ])\n",
    "\n",
    "            img = transformForReplicas(img)\n",
    "            img = transformForReplicas2(img)\n",
    "        img = img.float()\n",
    "\n",
    "        \n",
    "        if self.purpose == 'test':\n",
    "            return img\n",
    "        else:\n",
    "            return img, self.labels[index]\n",
    "        return img, torch.tensor([self.labels[index]])\n",
    "\n",
    "\n",
    "        data_dict = {\n",
    "            'image': img,\n",
    "            'label': torch.tensor([self.labels[index]])\n",
    "        }\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        # if self.tfm_on_patch is None: return data_dict\n",
    "\n",
    "        # for tfm in self.tfm_on_patch:\n",
    "        #     data_dict = tfm(data_dict)\n",
    "\n",
    "        \n",
    "\n",
    "        return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HAM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/k.urbanczyk/Documents/datascience/datapractical/vit-siim/play.ipynb Cell 2'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/k.urbanczyk/Documents/datascience/datapractical/vit-siim/play.ipynb#ch0000027?line=0'>1</a>\u001b[0m base_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/dss/dssmcmlfs01/pn69za/pn69za-dss-0002/ra49tad2/data/ham\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/k.urbanczyk/Documents/datascience/datapractical/vit-siim/play.ipynb#ch0000027?line=1'>2</a>\u001b[0m train_ds \u001b[39m=\u001b[39m HAM(base_path, purpose\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m, seed\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m, split\u001b[39m=\u001b[39m\u001b[39m0.25\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'HAM' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "base_path = \"/dss/dssmcmlfs01/pn69za/pn69za-dss-0002/ra49tad2/data/ham\"\n",
    "train_ds = HAM(base_path, purpose='train', seed=42, split=0.25)\n",
    "\n",
    "\n",
    "\n",
    "# train, val = train_test_split(meta_df, test_size=split, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df['cancer_idx'] = pd.Categorical(meta_df['dx']).codes\n",
    "meta_df['image'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_part1 = os.listdir(f'{base_path}/HAM10000_images_part_1/')\n",
    "img_part2 = os.listdir(f'{base_path}/HAM10000_images_part_2/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('vit')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2cf8d89f9befaf1d28407f455a298e72210d4682f6824944c8efae7526d4df06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
