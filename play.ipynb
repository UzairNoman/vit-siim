{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "[INFO] In original paper, mlp_hidden(CURRENT:384) is set to: 1536(=384*4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "c:\\Users\\HP\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:733: LightningDeprecationWarning: `trainer.fit(train_dataloader)` is deprecated in v1.4 and will be removed in v1.6. Use `trainer.fit(train_dataloaders)` instead. HINT: added 's'\n",
      "  rank_zero_deprecation(\n",
      "\n",
      "  | Name      | Type              | Params\n",
      "------------------------------------------------\n",
      "0 | model     | ViT               | 6.3 M \n",
      "1 | criterion | BCEWithLogitsLoss | 0     \n",
      "------------------------------------------------\n",
      "6.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.3 M     Total params\n",
      "25.063    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment:vit_siim\n",
      "[INFO] Log with CSV\n",
      "Epoch 0:   0%|          | 0/2 [14:03:47<?, ?it/s]:24<00:00, 24.20s/it]\n",
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:412: UserWarning: The number of training samples (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/2 [00:00<?, ?it/s, loss=20.5, v_num=30]        "
     ]
    }
   ],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import argparse\n",
    "from sklearn import metrics\n",
    "import comet_ml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import pytorch_lightning as pl\n",
    "import warmup_scheduler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchmetrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from utils import get_model, get_dataset, get_experiment_name, get_criterion\n",
    "from da import CutMix, MixUp\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "class Settings:\n",
    "    def __init__(self):\n",
    "        self.dataset = \"siim\"\n",
    "        self.num_classes = 2\n",
    "        self.model_name = \"vit\"\n",
    "        self.patch = 8\n",
    "        self.batch_size = 128\n",
    "        self.eval_batch_size = 1024\n",
    "        self.lr = 1e-3\n",
    "        self.min_lr = 1e-5\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.999\n",
    "        self.max_epochs = 3\n",
    "        self.weight_decay = 5e-5\n",
    "        self.warmup_epoch = 5\n",
    "        self.precision = 16\n",
    "        self.criterion = \"ce\"\n",
    "        self.smoothing = 0.1\n",
    "        self.dropout = 0.0\n",
    "        self.head = 12\n",
    "        self.num_layers = 7\n",
    "        self.hidden = 384\n",
    "        self.label_smoothing = False\n",
    "        self.mlp_hidden = 384\n",
    "        self.seed = 42\n",
    "        self.project_name = \"VisionTransformer\"\n",
    "        self.off_benchmark = False\n",
    "        self.dry_run = False\n",
    "        self.autoaugment = False\n",
    "        self.rcpaste = False\n",
    "        self.cutmix = False\n",
    "        self.mixup = False\n",
    "        self.off_cls_token = False\n",
    "        self.api_key = False\n",
    "\n",
    "args = Settings()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "args.benchmark = True if not args.off_benchmark else False\n",
    "args.gpus = torch.cuda.device_count()\n",
    "args.num_workers = 4*args.gpus if args.gpus else 8\n",
    "args.is_cls_token = True if not args.off_cls_token else False\n",
    "if not args.gpus:\n",
    "    args.precision=32\n",
    "\n",
    "if args.mlp_hidden != args.hidden*4:\n",
    "    print(f\"[INFO] In original paper, mlp_hidden(CURRENT:{args.mlp_hidden}) is set to: {args.hidden*4}(={args.hidden}*4)\")\n",
    "train_ds, test_ds = get_dataset(args)\n",
    "# classes = torch.tensor([0, 1, 2])\n",
    "# indices = (torch.tensor(train_ds.targets)[..., None] == classes).any(-1).nonzero(as_tuple=True)[0]\n",
    "# train_ds = torch.utils.data.Subset(train_ds, indices)\n",
    "# test_ds = torch.utils.data.Subset(test_ds, indices)\n",
    "#print(data.shape)\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, pin_memory=True)\n",
    "test_dl = torch.utils.data.DataLoader(test_ds, batch_size=args.eval_batch_size, num_workers=args.num_workers, pin_memory=True)\n",
    "\n",
    "class Net(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super(Net, self).__init__()\n",
    "        # self.hparams = hparams\n",
    "        self.hparams.update(vars(hparams))\n",
    "        self.model = get_model(hparams)\n",
    "        self.criterion = get_criterion(args)\n",
    "        if hparams.cutmix:\n",
    "            self.cutmix = CutMix(hparams.size, beta=1.)\n",
    "        if hparams.mixup:\n",
    "            self.mixup = MixUp(alpha=1.)\n",
    "        self.log_image_flag = hparams.api_key is None\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.hparams.lr, betas=(self.hparams.beta1, self.hparams.beta2), weight_decay=self.hparams.weight_decay)\n",
    "        self.base_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=self.hparams.max_epochs, eta_min=self.hparams.min_lr)\n",
    "        self.scheduler = warmup_scheduler.GradualWarmupScheduler(self.optimizer, multiplier=1., total_epoch=self.hparams.warmup_epoch, after_scheduler=self.base_scheduler)\n",
    "        return [self.optimizer], [self.scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        img, label = batch\n",
    "        if self.hparams.cutmix or self.hparams.mixup:\n",
    "            if self.hparams.cutmix:\n",
    "                img, label, rand_label, lambda_= self.cutmix((img, label))\n",
    "            elif self.hparams.mixup:\n",
    "                if np.random.rand() <= 0.8:\n",
    "                    img, label, rand_label, lambda_ = self.mixup((img, label))\n",
    "                else:\n",
    "                    img, label, rand_label, lambda_ = img, label, torch.zeros_like(label), 1.\n",
    "            out = self.model(img)\n",
    "            loss = self.criterion(out, label)*lambda_ + self.criterion(out, rand_label)*(1.-lambda_)\n",
    "        else:\n",
    "            out = self(img)\n",
    "            loss = self.criterion(out[:,1], label.float())\n",
    "\n",
    "        if not self.log_image_flag and not self.hparams.dry_run:\n",
    "            self.log_image_flag = True\n",
    "            #self._log_image(img.clone().detach().cpu())\n",
    "\n",
    "        acc = torch.eq(out.argmax(-1), label).float().mean()\n",
    "        auc_score = metrics.roc_auc_score(label, out[:, 1].squeeze().detach().numpy())\n",
    "        self.log('auc', auc_score, on_step=True, on_epoch=True)\n",
    "        self.log('acc', acc, on_step=True, on_epoch=True)\n",
    "        self.log('loss', loss,on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        #self.log(\"lr\", self.optimizer.param_groups[0][\"lr\"], on_epoch=self.current_epoch)\n",
    "        self.log(\"lr\", self.optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        img, label = batch\n",
    "        out = self(img)\n",
    "        loss = self.criterion(out[:,1], label.float())\n",
    "        acc = torch.eq(out.argmax(-1), label).float().mean()\n",
    "        #self.log(\"val_loss\", loss)\n",
    "        #self.log(\"val_acc\", acc)\n",
    "\n",
    "        auc_score = metrics.roc_auc_score(label.cpu(), out[:, 1].squeeze())\n",
    "        self.log('auc', auc_score, on_step=True, on_epoch=True)\n",
    "        val_acc = torchmetrics.functional.accuracy(out[:, 1], label)\n",
    "        self.log('valid_acc_from_tmet', val_acc, on_step=True, on_epoch=True)\n",
    "        self.log('valid_acc', acc, on_step=True, on_epoch=True)\n",
    "        self.log('val_loss', loss,on_step=True, on_epoch=True)\n",
    "\n",
    "        fpr, tpr, thresholds = roc_curve(label, out[:, 1])\n",
    "        auc_rf = auc(fpr, tpr)\n",
    "        plt.figure(1)\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.plot(fpr, tpr, label='Vit (area = {:.3f})'.format(auc_rf))\n",
    "        plt.xlabel('False positive rate')\n",
    "        plt.ylabel('True positive rate')\n",
    "        plt.title('ROC curve')\n",
    "        plt.legend(loc='best')\n",
    "        self.logger.experiment.add_figure('AUC Curve', plt.gcf(), self.current_epoch)\n",
    "\n",
    "        return { 'loss': loss.item(), 'preds': out, 'target': label}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        preds = torch.cat([tmp['preds'] for tmp in outputs])\n",
    "        targets = torch.cat([tmp['target'] for tmp in outputs])\n",
    "        confusion_matrix = torchmetrics.functional.confusion_matrix(preds, targets, num_classes=args.num_classes)\n",
    "\n",
    "        df_cm = pd.DataFrame(confusion_matrix.cpu().numpy(), index = range(args.num_classes), columns=range(args.num_classes))\n",
    "        plt.figure(figsize = (args.num_classes,args.num_classes*2))\n",
    "        fig_ = sns.heatmap(df_cm, annot=True, cmap='Spectral').get_figure()\n",
    "        plt.close(fig_)\n",
    "        \n",
    "        self.logger.experiment.add_figure(\"Confusion matrix\", fig_, self.current_epoch)\n",
    "\n",
    "        fpr, tpr, thresholds = roc_curve(targets, preds[:, 1])\n",
    "        auc_rf = auc(fpr, tpr)\n",
    "        plt.figure(1)\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.plot(fpr, tpr, label='Vit (area = {:.3f})'.format(auc_rf))\n",
    "        plt.xlabel('False positive rate')\n",
    "        plt.ylabel('True positive rate')\n",
    "        plt.title('ROC/AUC curve')\n",
    "        plt.legend(loc='best')\n",
    "        self.logger.experiment.add_figure('ROC/AUC Curve', plt.gcf(), self.current_epoch)\n",
    "\n",
    "        # repo_root = os.path.abspath(os.getcwd())\n",
    "        # data_root = os.path.join(repo_root, \"logs\")\n",
    "        # list_of_files = glob.glob(f'{data_root}/*') # * means all if need specific format then *.csv\n",
    "        # latest_file = max(list_of_files, key=os.path.getctime)\n",
    "        # writer = SummaryWriter(latest_file)\n",
    "        # writer.add_figure(\"Confusion matrix\", fig_, self.current_epoch)\n",
    "\n",
    "    # def _log_image(self, image):\n",
    "    #     grid = torchvision.utils.make_grid(image, nrow=4)\n",
    "    #     self.logger.experiment.log_image(grid.permute(1,2,0))\n",
    "    #     print(\"[INFO] LOG IMAGE!!!\")\n",
    "\n",
    "\n",
    "\n",
    "experiment_name = get_experiment_name(args)\n",
    "if args.api_key:\n",
    "    print(\"[INFO] Log with Comet.ml!\")\n",
    "    logger = pl.loggers.CometLogger(\n",
    "        api_key=args.api_key,\n",
    "        save_dir=\"logs\",\n",
    "        project_name=args.project_name,\n",
    "        experiment_name=experiment_name\n",
    "    )\n",
    "    refresh_rate = 0\n",
    "else:\n",
    "    print(\"[INFO] Log with CSV\")\n",
    "    # logger = pl.loggers.CSVLogger(\n",
    "    #     save_dir=\"logs\",\n",
    "    #     name=experiment_name\n",
    "    # )\n",
    "    logger = TensorBoardLogger(name=\"vit_siim\",save_dir=\"logs\")\n",
    "    refresh_rate = 1\n",
    "net = Net(args)\n",
    "trainer = pl.Trainer(precision=args.precision,fast_dev_run=args.dry_run, gpus=args.gpus, benchmark=args.benchmark,logger=logger, max_epochs=args.max_epochs)\n",
    "trainer.fit(model=net, train_dataloader=train_dl, val_dataloaders=test_dl)\n",
    "if not args.dry_run:\n",
    "    model_path = f\"weights/{experiment_name}.pth\"\n",
    "    torch.save(net.state_dict(), model_path)\n",
    "    if args.api_key:\n",
    "        logger.experiment.log_asset(file_name=experiment_name, file_data=model_path)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dc07d24e2f18896857f0b2a651fe84ba40ce7b297e58d8804a308c8039f752a6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
